{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLlBkMPzJ9hk"
      },
      "source": [
        "# __Handling multiple sequences (PyTorch)__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq2SjD-JJ9hn"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1op9f5YtJ9hn"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyF2NJ7UKIKa"
      },
      "source": [
        "## –ú–æ–¥–µ–ª–∏ –æ–∂–∏–¥–∞—é—Ç –ø–∞–∫–µ—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVbBXtgQLd98"
      },
      "source": [
        "–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è –≤ —Å–ø–∏—Å–∫–∏ —á–∏—Å–µ–ª. –î–∞–≤–∞–π—Ç–µ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —ç—Ç–æ—Ç —Å–ø–∏—Å–æ–∫ —á–∏—Å–µ–ª –≤ —Ç–µ–Ω–∑–æ—Ä –∏ –æ—Ç–ø—Ä–∞–≤–∏–º –µ–≥–æ –≤ –º–æ–¥–µ–ª—å:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8XBYUy8wJ9ho"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1adX851uL8g_",
        "outputId": "8ee5411f-b795-467f-d380-c9e48747b1d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.']\n",
            "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
            "tensor([ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "         2026,  2878,  2166,  1012])\n"
          ]
        }
      ],
      "source": [
        "print(tokens)\n",
        "print(ids)\n",
        "print(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sP_VKYNkL6Vo"
      },
      "outputs": [],
      "source": [
        "# This line will fail.\n",
        "model(input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKcfH1rjOpfw"
      },
      "source": [
        "#### –û—à–∏–±–∫–∞!!!  (IndexError: too many indices for tensor of dimension 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<small>—É `input_ids` —Ñ–æ—Ä–º–∞ (sequence_length,) ‚Äî –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä (–Ω–∞–ø—Ä–∏–º–µ—Ä, [12, 345, 6789]), –∞ model \n",
        "`distilbert-base-uncased-finetuned-sst-2-english` –æ–∂–∏–¥–∞–µ—Ç –¥–≤—É–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é (batch_size, sequence_length).\n",
        "\n",
        "<font color='lightgreen'>! –†–∞–Ω–µ–µ –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –º–æ–¥–µ–ª—å  \n",
        "`bert-base-cased` ‚Äî —ç—Ç–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π BERT —Å torch.nn.Module, –∫–æ—Ç–æ—Ä—ã–π —É–º–µ–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã (sequence_length,). –ú–æ–¥–µ–ª—å bert-base-cased —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ —Ç–∞–∫, —á—Ç–æ –µ—Å–ª–∏ —Ç—ã –ø–µ—Ä–µ–¥–∞—ë—à—å –µ–π –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä (sequence_length,), –æ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç batch —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–Ω—É—Ç—Ä–∏ forward().  \n",
        "`distilbert-base-uncased-finetuned-sst-2-english` ‚Äî —ç—Ç–æ –æ–±–ª–µ–≥—á—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è DistilBERT, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∏ –æ–Ω–∞ —Ç—Ä–µ–±—É–µ—Ç (batch_size, sequence_length).</font>\n",
        "\n",
        "__–ö–∞–∫ –∏—Å–ø—Ä–∞–≤–∏—Ç—å?__  \n",
        "–î–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å batch (—Ä–∞–∑–º–µ—Ä 1), –∏—Å–ø–æ–ª—å–∑—É—è `.unsqueeze(0):`  \n",
        "`input_ids = torch.tensor(ids).unsqueeze(0)  # –¢–µ–ø–µ—Ä—å —Ñ–æ—Ä–º–∞ (1, sequence_length)`\n",
        "\n",
        "__–õ—É—á—à–∏–π —Å–ø–æ—Å–æ–± ‚Äì –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å tokenizer()__</small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwoLAbm8J9hp",
        "outputId": "c634f8ea-6f09-42ad-aa29-313e9d165e58"
      },
      "outputs": [],
      "source": [
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "# print(tokenized_inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "–ó–¥–µ—Å—å –º—ã –º–æ–∂–µ–º –≤–∏–¥–µ—Ç—å, —á—Ç–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –£–±–∏—Ä–∞–µ–º batch —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏ –≤–∏–¥–∏–º –æ–¥–Ω–æ–º–µ—Ä–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å\n",
        "tokens_2 = tokenizer(sequence, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)  \n",
        "print(\"Input shape:\", tokens_2.shape)  # (sequence_length,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "–ê –≤–æ—Ç —É–∂–µ `tokenized_inputs` –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–æ—Å—Ç–∞–≤ –∏ —Ñ–æ—Ä–º—É –∏ –ø–æ–¥–∞–≤ –µ–≥–æ –Ω–∞ –≤—Ö–æ–¥ –º–æ–¥–µ–ª–∏ –æ—à–∏–±–∫–∞ –Ω–µ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "Logits: tensor([[-1.5607,  1.6123]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "output = model(**tokenized_inputs)\n",
        "print(output)\n",
        "print(\"Logits:\", output.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ2__JTAPbgm"
      },
      "source": [
        "–ü–æ–ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–º —Å–ø–æ—Å–æ–±–æ–º –∏ –æ–Ω —Ç–æ–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "tokens_3 = tokenizer.tokenize(sequence)\n",
        "ids_3 = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids_3 = torch.tensor([ids_3])\n",
        "output = model(input_ids_3)\n",
        "print(\"Logits:\", output.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "–í—Ä–æ–¥–µ –±—ã –∫–æ–¥\n",
        "(1)\n",
        "``` \n",
        "tokens_3 = tokenizer.tokenize(sequence)\n",
        "ids_3 = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids_3 = torch.tensor([ids_3])\n",
        "```\n",
        "–¥–µ–ª–∞–µ—Ç —Ç–æ-–∂–µ, —á—Ç–æ –∏ \n",
        "(2)\n",
        "```\n",
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "```\n",
        "–ù–û!  \n",
        "__–†–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É inputs_1 –∏ input_2__  \n",
        "```\n",
        "–§—É–Ω–∫—Ü–∏—è\t                        (1)\t                    (2)\n",
        "–§–æ—Ä–º–∞\t                        (1, sequence_length)    (1, sequence_length + 2) (–∏–∑-–∑–∞ [CLS] –∏ [SEP])\n",
        "–î–æ–±–∞–≤–ª—è–µ—Ç [CLS] –∏ [SEP]?\t‚ùå –ù–µ—Ç                  ‚úÖ –î–∞      \n",
        "–î–æ–±–∞–≤–ª—è–µ—Ç attention_mask?\t‚ùå –ù–µ—Ç                  ‚úÖ –î–∞\t\n",
        "–ì–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ –º–æ–¥–µ–ª–∏?\t‚ùå –ù–µ—Ç                  ‚úÖ –î–∞\t\n",
        "```\n",
        "üéØ –ò—Ç–æ–≥:  \n",
        "`tokenizer(sequence, return_tensors=\"pt\")` = —É–¥–æ–±–Ω—ã–π —Å–ø–æ—Å–æ–±, –æ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç [CLS], [SEP], attention_mask –∏ –¥–µ–ª–∞–µ—Ç —Ç–µ–Ω–∑–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã.  \n",
        "–†—É—á–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è = —Ç—ã –¥–æ–ª–∂–µ–Ω —Å–∞–º –¥–æ–±–∞–≤–∏—Ç—å `[CLS], [SEP]` –∏ `attention_mask`, –∏–Ω–∞—á–µ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ.  \n",
        "–õ—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `tokenizer(sequence, return_tensors=\"pt\")`, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫. ‚úÖ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### –û–±—Ä–∞–±–æ—Ç–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –≤–∏–¥–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1ISsIsMQYgw"
      },
      "source": [
        "–ü–∞–∫–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ‚Äî —ç—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –æ—Ç–ø—Ä–∞–≤–∫–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å, –≤—Å–µ —Å—Ä–∞–∑—É.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "sequence_2 = [\"I've been waiting for a HuggingFace course my whole life.\", \n",
        "\"I  wait when a HuggingFace will be better.\",\n",
        "\"I love HuggingFace\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "–ú—ã –¥–æ–ª–∂–Ω—ã –ø–æ–¥–∞—Ç—å –º–∞—Ç—Ä–∏—Ü—É, –∏–ª–∏ –º–∞—Å—Å–∏–≤ –¥–∞–Ω–Ω—ã—Ö –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã, —á—Ç–æ –ø—Ä–µ–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω—ã. –î–ª—è —ç—Ç–æ–≥–æ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º `padding=True` –≤ –ø—Ä–æ—Ü–µ–¥—É—Ä–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
              "          2607,  2026,  2878,  2166,  1012,   102],\n",
              "        [  101,  1045,  3524,  2043,  1037, 17662, 12172,  2097,  2022,  2488,\n",
              "          1012,   102,     0,     0,     0,     0],\n",
              "        [  101,  1045,  2293, 17662, 12172,   102,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenize_input_2 = tokenizer(sequence_2, padding=True, return_tensors=\"pt\")\n",
        "tokenize_input_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.5607,  1.6123],\n",
              "        [ 2.8782, -2.4049],\n",
              "        [-3.8986,  4.1859]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_2 = model(**tokenize_input_2)\n",
        "output_2.logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhO8ck1qUk9M"
      },
      "source": [
        "## Longer sequences (–ë–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uegj9YHvUmHM"
      },
      "source": [
        "–ë–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "–í –º–æ–¥–µ–ª—è—Ö Transformer —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –º–æ–∂–µ–º –ø–µ—Ä–µ–¥–∞—Ç—å –º–æ–¥–µ–ª—è–º. –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω–æ–π –¥–æ `512` –∏–ª–∏ `1024` —Ç–æ–∫–µ–Ω–æ–≤ –∏ –±—É–¥—É—Ç –∞–≤–∞—Ä–∏–π–Ω–æ –∑–∞–≤–µ—Ä—à–∞—Ç—å —Ä–∞–±–æ—Ç—É –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π. –ï—Å—Ç—å –¥–≤–∞ —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –ø—Ä–æ–±–ª–µ–º—ã:\n",
        "\n",
        "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª—å —Å –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ–π –¥–ª–∏–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
        "–£—Å–µ–∫–∞–π—Ç–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\n",
        "–ú–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç —Ä–∞–∑–Ω—É—é –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—É—é –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π.  \n",
        "- Longformer ‚Äî –æ–¥–∏–Ω –∏–∑ –ø—Ä–∏–º–µ—Ä–æ–≤, –∞ –¥—Ä—É–≥–æ–π\n",
        "‚Äî LED. –ï—Å–ª–∏ –≤—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ –Ω–∞–¥ –∑–∞–¥–∞—á–µ–π, —Ç—Ä–µ–±—É—é—â–µ–π –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –º—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –≤–∞–º –≤–∑–≥–ª—è–Ω—É—Ç—å –Ω–∞ —ç—Ç–∏ –º–æ–¥–µ–ª–∏.\n",
        "\n",
        "–í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ –º—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –≤–∞–º —É—Å–µ–∫–∞—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —É–∫–∞–∑–∞–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä max_sequence_length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2pLwrqaJ9hs"
      },
      "outputs": [],
      "source": [
        "sequence = sequence[:max_sequence_length]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Handling multiple sequences (PyTorch)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
