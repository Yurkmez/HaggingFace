{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# __Fine-tuning a masked language model (PyTorch)__"
      ],
      "metadata": {
        "id": "-y2zBZ9sDA0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Import, pip, ets."
      ],
      "metadata": {
        "id": "Ff49JeLDC6Xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "dfK5iU5GCAEg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "ae022ce575114d1e9c5c6035af855083",
            "f17e56a195d94676bd7fd749757d0216",
            "65ed6df8ca21481599264c5aa83022d3",
            "d86de6945ab34774bc6da09983f62c9f",
            "dbf119930e2f41208c63638eee7583db",
            "12c1754d08da4a52b3b6149920c8a378",
            "7aed193c44e547ce8206fe6ebde2ad9d",
            "d63845b707fd4e1dabdf03b4bc0202ff",
            "226d54e516fb445287309a3b6757e79c",
            "e6f3f6f316e0465faca33af8846114ad",
            "f0cbdf558bb842c998c770298c292ef0",
            "fe3b927fb7fe4493a5411d4eb75ddb5d",
            "340bde4ab02f4c148006251ec1c703ae",
            "4fec027ef9b4476f83783524f36e0787",
            "c85b1d62c335425f914b23f228371e42",
            "a57c624fc36749d980b765ede06d34af",
            "df331bfbe9b842faa0cf6a0a7e9124c0",
            "deb05a588b62435d8a78b851d033fb4c",
            "bb7eb687386240f9b36422629e03e8b6",
            "3e4b4be8d8834dcc94b983d591f8c691"
          ]
        },
        "outputId": "c79377c6-ee06-4343-9f7e-a0433c585d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae022ce575114d1e9c5c6035af855083"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sIScQXJhCFfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/My Drive/Hugging_face/bert-finetuned-ner\""
      ],
      "metadata": {
        "id": "eqqBM7_ICHzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the following line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!apt install git-lfs"
      ],
      "metadata": {
        "id": "2eBg-HiwCR8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from huggingface_hub import model_info\n",
        "import torch\n",
        "# from huggingface_hub import list_models"
      ],
      "metadata": {
        "id": "_kY6DkLnCXh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Загрузка модели и токенизатора"
      ],
      "metadata": {
        "id": "er8L7CXHCKUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### text"
      ],
      "metadata": {
        "id": "BQoZZjEJjNG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для многих приложений NLP, включающих модели Transformer, годится предварительно обученная модель из Hugging Face.  \n",
        "Однако иногда  нужно настроить языковые модели на ваших данных, прежде чем обучать конкретную для задачи голову. Например, если ваш набор данных содержит юридические контракты или научные статьи, ванильная модель Transformer, такая как BERT, обычно будет рассматривать доменно-специфические слова в вашем корпусе как редкие токены, и результирующая производительность может быть неудовлетворительной. Тонкая настройка языковой модели на внутридоменных данных позволяет повысить производительность!  \n",
        "Этот процесс тонкой настройки предварительно обученной языковой модели на внутридоменных данных обычно называется адаптацией домена."
      ],
      "metadata": {
        "id": "AtZ777qSy6Mx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Хотя семейства моделей BERT и RoBERTa являются наиболее скачиваемыми, мы будем использовать модель под названием DistilBERT, которая может обучаться гораздо быстрее с небольшими потерями в производительности в дальнейшем. Эта модель была обучена с использованием специальной техники, называемой дистилляцией знаний, где большая «модель учителя», такая как BERT, используется для руководства обучением «модели ученика», которая имеет гораздо меньше параметров.  \n",
        "Давайте продолжим и загрузим DistilBERT с помощью класса AutoModelForMaskedLM:"
      ],
      "metadata": {
        "id": "50ofpf_h0TMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### code"
      ],
      "metadata": {
        "id": "Qyc507l9jRfZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkHEyCNWySXI"
      },
      "outputs": [],
      "source": [
        "# !git config --global user.email \"you@example.com\"\n",
        "# !git config --global user.name \"Your Name\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rublj6ktySXI"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpTTanEWySXJ"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTJnQpRxySXJ",
        "outputId": "5032a4b6-2f0a-4616-a0f5-301818594bae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'>>> DistilBERT number of parameters: 67M'\n"
          ]
        }
      ],
      "source": [
        "# Мы можем увидеть, сколько параметров имеет эта модель, вызвав метод num_parameters():\n",
        "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
        "print(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\") # DistilBERT number of parameters: 67M (BERT  ~ 110M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0oTr-xJySXK"
      },
      "outputs": [],
      "source": [
        "text = \"This is a great [MASK].\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### text"
      ],
      "metadata": {
        "id": "2V_RjDPrlnRs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как люди, мы можем представить себе множество возможностей для токена [MASK], например, «день», «езда» или «живопись». Для предварительно обученных моделей прогнозы зависят от корпуса, на котором была обучена модель, поскольку она учится улавливать статистические закономерности, присутствующие в данных. Как и BERT, DistilBERT был предварительно обучен на наборах данных английской Wikipedia и BookCorpus, поэтому мы ожидаем, что прогнозы для [MASK] будут отражать эти домены. Чтобы предсказать маску, нам нужен токенизатор DistilBERT для создания входных данных для модели, поэтому давайте загрузим его также из Hub:"
      ],
      "metadata": {
        "id": "1-fcCdcS1rdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### code"
      ],
      "metadata": {
        "id": "jlQU2WtjlqYF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2twi51uoySXK"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) # превращает текст в тензор формата PyTorch (pt), который можно передавать в модель."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "преобразовываем текст в тензор формата PyTorch (pt), подготовленный для передачи в модель."
      ],
      "metadata": {
        "id": "ckB2PpTunkKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "inputs"
      ],
      "metadata": {
        "id": "0xgJ7ZK42GWs",
        "outputId": "b55bda2b-0eb7-4382-925b-7d7f60478bab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 2307,  103, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используя токенизатор и модель, передаем text в модель"
      ],
      "metadata": {
        "id": "TMsFcXgQmby6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_logits = model(**inputs).logits # Получение логитов (сырых вероятностей) токенов от модели:\n",
        "# model(**inputs) — передает токенизированный текст в модель.\n",
        "# .logits — извлекает логиты (до softmax), предсказанные моделью."
      ],
      "metadata": {
        "id": "bxJLvtCK2GPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdrR1NSZnyGH",
        "outputId": "e030fa9c-8a09-403b-e38d-01fa026df44a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 30522])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### text (__Формат torch.Size([batch_size, sequence_length, vocab_size])__"
      ],
      "metadata": {
        "id": "aQ_j6LvMmuEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `batch size` - (размер батча), означает, что в тензоре содержится один пример (один входной текст).  Если torch.Size([32, 8, 30522]) - 32 примера.  \n",
        "— `sequence length` - (длина последовательности токенов), количество токенов в обработанном тексте. Например, \"Hello, how are you?\" получил 8 токенов.\n",
        "- `vocab size` - размер словаря модели. Это количество возможных токенов в словаре модели. Например, для BERT-base стандартный размер словаря — 30522 токена.  \n",
        "- Каждое значение в тензоре представляет логиты (сырые вероятности) для каждого токена из словаря."
      ],
      "metadata": {
        "id": "Elx1li1UqjqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Как интерпретировать этот тензор?__\n",
        "Этот тензор содержит логиты _для предсказания_ каждого токена в последовательности. _Для каждого из них есть вектор размером 30522_, где каждое значение соответствует вероятности (до softmax) выбора данного токена из словаря."
      ],
      "metadata": {
        "id": "NgvywtwLrneG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_logits[0][7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQdY5Ctmnaj8",
        "outputId": "31416d13-7959-4fc5-e8dd-27f327dbbb36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-9.5213, -9.4632, -9.5022,  ..., -8.6561, -8.4908, -4.6903],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_logits[0][7].shape # Вектор логитов для 8-го токена в тексте"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YEvl2SwnkAP",
        "outputId": "20d3b1a8-2086-40f7-ff82-c9d0c3da0d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([30522])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### code"
      ],
      "metadata": {
        "id": "VEIM2EQ6tMxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the location of [MASK] and extract its logits (text = \"This is a great [MASK].\")\n",
        "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
        "print(mask_token_index)\n",
        "print(tokenizer.mask_token_id)"
      ],
      "metadata": {
        "id": "iPjZIWm22GFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "482249c0-ff10-4a2d-bf51-f56c74c96db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([5])\n",
            "103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### text (__Что делает код__ `mask_token_index = ...`  )"
      ],
      "metadata": {
        "id": "2eJ7lOH_pHQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `inputs[\"input_ids\"]` — это числовые идентификаторы токенов входного текста.\n",
        "```\n",
        "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 2307,  103, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
        "```\n",
        "- `tokenizer.mask_token_id` — ID маскированного токена ([MASK]). Когда в тексте присутствует [MASK], он заменяется специальным индексом, который можно получить через tokenizer.mask_token_id. В зависимости от используемого токенизатора (например, BERT, RoBERTa, DistilBERT) tokenizer.mask_token_id имеет фиксированное числовое значение. Например, для BERT - это 103, для DistilBERT (как у нас) - аналогично, 103. Когда этот тензор передается в модель: token_logits = model(**inputs).logits, модель предсказывает вероятности всех 30522 возможных токенов на месте 103 (то есть [MASK]). Дальше можно выбрать самые вероятные слова и подставить их вместо [MASK].  \n",
        "- `torch.where(... == ...)` находит индекс (позицию) [MASK] в последовательности (то есть, у нас 8 токенов, на позициях 0, 1, 2, ..., и [MASK] у нас на позиции 5."
      ],
      "metadata": {
        "id": "TThQMxeAtQdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### code"
      ],
      "metadata": {
        "id": "0GMW90-SpTLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "mask_token_logits.shape"
      ],
      "metadata": {
        "id": "v4stkLUt2F0q",
        "outputId": "6d20a2f5-3a26-4902-b1b4-3b8dd036d1c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 30522])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### text (Что делает код: `mask_token_logits = ...` )"
      ],
      "metadata": {
        "id": "VI0cqvqdpv6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- `token_logits[0]` — логиты для первого (и единственного) элемента батча.  \n",
        "- `mask_token_index` — индекс [MASK].  \n",
        "- `:` — означает, что берем все предсказания для этого токена (логиты всех возможных слов)."
      ],
      "metadata": {
        "id": "sWCpOiwAAF-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### code"
      ],
      "metadata": {
        "id": "SQy0ZX4fp5hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick the [MASK] candidates with the highest logits\n",
        "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist() # выбирает 5 наибольших логитов вдоль измерения 1 (по токенам).\n",
        "# .indices[0].tolist() — берет их индексы и превращает в список.\n",
        "print(top_5_tokens)\n",
        "for token in top_5_tokens:\n",
        "  print(tokenizer.decode([token]))\n",
        "print(\"____________________\")\n",
        "for token in top_5_tokens:\n",
        "  print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")\n",
        "# tokenizer.decode([token]) - превращает токен обратно в слово.\n",
        "# text.replace(tokenizer.mask_token, ...)` - заменяет [MASK] на предсказанное слово."
      ],
      "metadata": {
        "id": "pZY5tXLJ2qsl",
        "outputId": "046fa6ca-010e-4d5f-e651-31fd01994866",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3066, 3112, 6172, 2801, 8658]\n",
            "deal\n",
            "success\n",
            "adventure\n",
            "idea\n",
            "feat\n",
            "____________________\n",
            "'>>> This is a great deal.'\n",
            "'>>> This is a great success.'\n",
            "'>>> This is a great adventure.'\n",
            "'>>> This is a great idea.'\n",
            "'>>> This is a great feat.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Из результатов видно, что прогнозы модели относятся к повседневным терминам, что, возможно, неудивительно, учитывая основу английской Википедии. Давайте посмотрим, как мы можем изменить эту область на что-то более узкоспециализированное — высокополяризованные обзоры фильмов!"
      ],
      "metadata": {
        "id": "wNvQ2nKiBqFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Адаптация (тонкая настройка) обученной на \"обыденных\" данных (Википедия) модели на выполнении специальной задачи"
      ],
      "metadata": {
        "id": "B6wearuhByec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Загрузка и просмотр данных"
      ],
      "metadata": {
        "id": "xXYZMtlxNVGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### text"
      ],
      "metadata": {
        "id": "RR9RmBojfd_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Будем использовать Large Movie Review Dataset (IMDb), который представляет собой корпус обзоров фильмов, часто используемый для сравнения моделей анализа настроений. Тонко настраивая DistilBERT на этом корпусе, мы ожидаем, что языковая модель адаптирует свой словарь из фактических данных Википедии, на которых она была предварительно обучена, к более субъективным элементам обзоров фильмов. Мы можем получить данные из Hugging Face Hub с помощью функции load_dataset() из 🤗 Наборы данных:"
      ],
      "metadata": {
        "id": "QfVT6rosC19w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### code"
      ],
      "metadata": {
        "id": "FEEUo3vOfhFK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Aq2NOKDySXK"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "imdb_dataset # Видно, что обучающий и тестовый сплиты состоят из 25 000 отзывов, немаркированный сплит, неконтролируемый -  содержит 50 000 отзывов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "po2LW-EZySXK"
      },
      "outputs": [],
      "source": [
        "# Рассмотрим несколько образцов из маркированных данных, создав случайную выборку и применив функции Dataset.shuffle() и Dataset.select()\n",
        "sample = imdb_dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
        "\n",
        "for row in sample:\n",
        "    print(f\"\\n'>>> Review: {row['text']}'\")\n",
        "    print(f\"'>>> Label: {row['label']}'\")\n",
        "    # видим, что 0 обозначает отрицательный отзыв, а 1 соответствует положительному."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Из немаркированных данных\n",
        "sample = imdb_dataset[\"unsupervised\"].shuffle(seed=42).select(range(3))\n",
        "\n",
        "for row in sample:\n",
        "    print(f\"\\n'>>> Review: {row['text']}'\")\n",
        "    print(f\"'>>> Label: {row['label']}'\")\n",
        "    # видим, что у неразмеченных данных Label: -1' - что означает, что разметки нет."
      ],
      "metadata": {
        "id": "AlPTv4cPFomq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Предварительная обработка данных"
      ],
      "metadata": {
        "id": "xWFgm0H6GntK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### text"
      ],
      "metadata": {
        "id": "rhHXd8qygyfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизируем наш корпус, но без установки параметра truncation=True в нашем токенизаторе.   \n",
        "<small>(Как для авторегрессивного, так и для маскированного языкового моделирования общим этапом предварительной обработки является объединение всех примеров, а затем разделение всего корпуса _на фрагменты одинакового размера_, так как отдельные примеры могут быть усечены, если они слишком длинные, и это приведет к потере информации, которая может быть полезна для задачи языкового моделирования!)</small>  \n",
        "Мы также захватим идентификаторы слов, если они доступны (что будет, если мы используем быстрый токенизатор, как описано в Главе 6), так как они нам понадобятся позже для маскировки целых слов. Мы обернем это в простую функцию, и удалим столбцы текста и меток, поскольку они нам больше не нужны:"
      ],
      "metadata": {
        "id": "b0qoU_AgHdNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"yellow\">Напомним, что у нас:\n",
        "- model_checkpoint = \"distilbert-base-uncased\"\n",
        "- model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
        "- tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)</font>\n"
      ],
      "metadata": {
        "id": "O4uxhZoOI9_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### code"
      ],
      "metadata": {
        "id": "bTpkO_XHg2_V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XewrjYRSySXL"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    result = tokenizer(examples[\"text\"])\n",
        "    # print(result[\"input_ids\"])\n",
        "    # print(len(result[\"input_ids\"]))\n",
        "    if tokenizer.is_fast:\n",
        "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
        "        # print(result[\"word_ids\"])\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### text (__result[\"word_ids\"] = ...__)  "
      ],
      "metadata": {
        "id": "86SNeNSLiVDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "используется метод __word_ids()__, который доступен только для fast-токенизаторов (основанных на tokenizers из библиотеки Hugging Face).\n",
        "Что делает эта строка?  \n",
        "- `result[\"input_ids\"]` — список токенов после токенизации.  \n",
        "- `len(result[\"input_ids\"])` — количество примеров в батче (сколько отдельных текстов было токенизировано).\n",
        "- `result.word_ids(i)` -  возвращает список индексов исходных слов для каждого под-токена в i-той последовательности (i — номер примера в батче).\n",
        "Если слово состоит из нескольких под-токенов, все эти под-токены будут привязаны к одному и тому же word_id.  \n",
        "- `[result.word_ids(i) for i in range(len(result[\"input_ids\"]))]` создается список, в котором для каждого примера в батче (i) вызывается result.word_ids(i).  \n",
        "Итоговый список содержит списки word_ids для каждого текста."
      ],
      "metadata": {
        "id": "TST32jlbJZvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример работы  \n",
        "`text = [\"I love NLP!\", \"Tokenization is fun.\"]`  \n",
        "После токенизации (примерные input_ids):  \n",
        "```\n",
        "[\n",
        "    [101, 1045, 2293, 17953, 999, 102],  # \"I love NLP !\"\n",
        "    [101, 19204, 2003, 4569, 1012, 102]  # \"Tokenization is fun .\"\n",
        "]  \n",
        "```\n",
        "[101] и [102] — специальные токены [CLS] и [SEP].  \n",
        "\"NLP\" токенизируется в один токен 17953.  \n",
        "\"Tokenization\" может разделиться на [\"Token\", \"ization\"].  \n",
        "Результат word_ids():  \n",
        "```\n",
        "[\n",
        "    [None, 0, 1, 2, 3, None],  # Для \"I love NLP !\"\n",
        "    [None, 0, 1, 2, 3, None]   # Для \"Tokenization is fun .\"\n",
        "]\n",
        "```\n",
        "None → специальные токены [CLS] и [SEP] (не относятся ни к одному слову).  \n",
        "Остальные цифры (0, 1, 2, ...) → соответствуют индексам исходных слов:  \n",
        "0 → \"I\"  \n",
        "1 → \"love\"  \n",
        "2 → \"NLP\"  \n",
        "3 → \"!\"  \n",
        "Аналогично для второго предложения.  \n",
        "Зачем это нужно?  \n",
        "word_ids позволяют отслеживать, к каким исходным словам относятся под-токены.\n",
        "Это полезно для:  \n",
        "Выравнивания аннотаций (например, при разметке Named Entity Recognition, NER).\n",
        "Обратного преобразования токенов в слова.  \n",
        "Анализа субтокенов (например, \"Tokenization\" разбивается на [\"Token\", \"ization\"], но относится к одному word_id).  \n",
        "Вывод  \n",
        "`result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]` добавляет в result список word_ids, который помогает отслеживать, какие под-токены относятся к каким исходным словам.  \n",
        "Поскольку DistilBERT — это модель, подобная BERT, мы видим, что закодированные тексты состоят из `input_ids, attention_mask`, а также `word_ids`, которые мы добавили.  "
      ],
      "metadata": {
        "id": "8bGFxYLpMiot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### code"
      ],
      "metadata": {
        "id": "Ow6n4z2Wikup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим, что делает ф-ция __tokenize_function__ (на примере 2-х примеров из датасета)"
      ],
      "metadata": {
        "id": "KBeSwMXwr5io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use batched=True to activate fast multithreading!\n",
        "tokenized_datasets_example = imdb_dataset[\"train\"].select(range(1)).map(\n",
        "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
        ")\n",
        "tokenized_datasets_example\n",
        "# [[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, ...]]"
      ],
      "metadata": {
        "id": "h_H9Qn4Hh9LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Итак, добавим в result список word_ids, который помогает отслеживать, какие под-токены относятся к каким исходным словам."
      ],
      "metadata": {
        "id": "Oq8oKZDNyvOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use batched=True to activate fast multithreading!\n",
        "tokenized_datasets = imdb_dataset.map(\n",
        "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
        ")\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "IT2llFjwh_T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjUfs_MeySXL",
        "outputId": "d71b502b-f2a3-415a-8ea3-5d83fd860fe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "tokenizer.model_max_length"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### text"
      ],
      "metadata": {
        "id": "8_5c_KhtzYhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь, когда мы токенизировали наши обзоры фильмов, следующим шагом будет сгруппировать их все вместе и разбить результат на фрагменты. Это в конечном итоге будет определяться объемом памяти GPU, который у нас есть, и хорошей отправной точкой является максимальный размер контекста модели, который мы узнали выше (.model_max_length токенизатора из файла tokenizer_config.json, связанного с контрольной точкой. Видим, что размер контекста составляет 512 токенов, как и в BERT.\n",
        "\n",
        "✏️ Некоторые модели Transformer, такие как BigBird и Longformer, имеют гораздо большую длину контекста, чем BERT и другие ранние модели Transformer."
      ],
      "metadata": {
        "id": "l9mI1HlwOHRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы запустить наши эксперименты на графических процессорах, выберем что-то немного меньшее, что может поместиться в памяти:"
      ],
      "metadata": {
        "id": "9A_9MNwl0E2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### code"
      ],
      "metadata": {
        "id": "dDCglQM60i8B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xINYhlGmySXL"
      },
      "outputs": [],
      "source": [
        "chunk_size = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Возьмем несколько отзывов из токенизированного обучающего набора и выведем количество токенов:"
      ],
      "metadata": {
        "id": "zWjeFsS4ORsA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnxQ8ci9ySXL",
        "outputId": "d4e8c5f9-b468-4396-fdbf-dd134f0083c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'>>> Review 0 length: 174'\n",
            "'>>> Review 1 length: 152'\n",
            "'>>> Review 2 length: 149'\n",
            "'>>> Review 3 length: 172'\n",
            "'>>> Review 4 length: 163'\n"
          ]
        }
      ],
      "source": [
        "# Slicing produces a list of lists for each feature\n",
        "tokenized_samples = tokenized_datasets[\"train\"][45:50]\n",
        "\n",
        "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
        "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Затем мы можем объединить все эти примеры с помощью простого словарного понимания следующим образом:"
      ],
      "metadata": {
        "id": "wgZQUneDOwgg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nN7jwDGuySXL",
        "outputId": "a369c2bd-df11-4a47-c8f5-fb820bb6e450",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'>>> Concatenated reviews length: 810'\n"
          ]
        }
      ],
      "source": [
        "concatenated_examples = {\n",
        "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
        "}\n",
        "total_length = len(concatenated_examples[\"input_ids\"])\n",
        "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### text (__как работает код k: sum(tokenized_samples[k], []) ...__)"
      ],
      "metadata": {
        "id": "0dyPbAbd15Zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Порядок выполнения кода\n",
        "- `for k in tokenized_samples.keys()` - вытаскивает из каждого примера (их 5) — ключи (\"input_ids\", \"attention_mask\", \"word_ids\").\n",
        "- `tokenized_samples[k]` — это список значений для каждого ключа k.\n",
        "- `sum(tokenized_samples[\"input_ids\"], [])` - объединяет их в один список,\n",
        "- `{\n",
        "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
        "}` - формирует уже словарь, в котором ключи как раз этои \"к\", а значения - объединенные списки.  \n",
        "\n",
        "Например:  \n",
        "```\n",
        "tokenized_samples = {\n",
        "    \"input_ids\": [[1, 2, 3], [4, 5], [6, 7, 8]],\n",
        "    \"attention_mask\": [[1, 1, 1], [1, 1], [1, 1, 1]]\n",
        "}\n",
        "```  \n",
        "После выполнения:  \n",
        "```\n",
        "concatenated_examples = {\n",
        "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
        "}\n",
        "```  \n",
        "получим:  \n",
        "```\n",
        "{\n",
        "    \"input_ids\": [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "    \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1]\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "jSrGsmeqdjEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### code"
      ],
      "metadata": {
        "id": "sjk_LATI2M7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Итак, общая длина проверена — разделим объединенные обзоры примера на фрагменты размера, заданного chunk_size. Результатом является словарь фрагментов для каждой функции:"
      ],
      "metadata": {
        "id": "nK7bhzPDf8_o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFMB4H60ySXM"
      },
      "outputs": [],
      "source": [
        "chunks = {\n",
        "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "    for k, t in concatenated_examples.items()\n",
        "}\n",
        "\n",
        "for chunk in chunks[\"input_ids\"]:\n",
        "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### text"
      ],
      "metadata": {
        "id": "ySBhUg2A4Uo7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как работает данный код  \n",
        "1. Итерация по `concatenated_examples.items()`\n",
        "Т.е., Берем каждую пару k, t для `input_ids`, `attention_mask` (`word_ids`), где k - ключ (это `input_ids`, `attention_mask` `word_ids`), t - список объединенных токенов.  \n",
        "Для каждого t создаем список кусков.  \n",
        "2. Используем range(0, total_length, chunk_size).  \n",
        "3. Разрезаем t на куски по chunk_size.  \n",
        "4. Формируем итоговый словарь chunks  \n",
        "Записываем разрезанные последовательности по соответствующим ключам."
      ],
      "metadata": {
        "id": "h4hE6Anom2qE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### code"
      ],
      "metadata": {
        "id": "XnKsktyC4bGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Последний фрагмент, как правило, меньше максимального размера фрагмента. Стратегии: отбросить, дополнить до chunk_size. Воспользуемся первым подходом. Обернем всю вышеприведенную логику в одну функцию и применим к токенизированным наборам данных:"
      ],
      "metadata": {
        "id": "6TsdZIGigWjm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hd-AHiMaySXM"
      },
      "outputs": [],
      "source": [
        "def group_texts(examples):\n",
        "    # Concatenate all texts\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    # Compute length of concatenated texts\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the last chunk if it's smaller than chunk_size\n",
        "    total_length = (total_length // chunk_size) * chunk_size\n",
        "    # Split by chunks of max_len\n",
        "    result = {\n",
        "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    # Create a new labels column\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    # Создание столбца labels (копия столбца input_ids) связано с тем,\n",
        "    # что в моделировании языка с масками цель состоит в том,\n",
        "    # чтобы предсказать случайно замаскированные токены во входном пакете,\n",
        "    # и, создавая столбец labels, мы предоставляем истину для обучения нашей языковой модели.\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь применим group_texts() к нашим токенизированным наборам данных с помощью функции Dataset.map():"
      ],
      "metadata": {
        "id": "UjjQ_opao9h1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='yellow'>Важно! Мы смешиваем все примеры в кучу, поэтому их разделять теперь не получится (например, по приниципу положительный, нейтральный или отрицательный отзыв), но мы решаем задачу МАСКИРОВКИ!, в этом случае мы просто берем массив текстов."
      ],
      "metadata": {
        "id": "8BGAhd4k5haT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--dtwE9aySXM"
      },
      "outputs": [],
      "source": [
        "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
        "lm_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Группировка и последующее разделение текстов на части дали гораздо больше примеров, чем наши исходные 25 000 для разделов обучения и тестирования. Это потому, что теперь у нас есть примеры, включающие смежные токены, которые охватывают несколько примеров из исходного корпуса. Можно увидеть это явно, посмотрев на специальные токены [SEP] и [CLS] в одном из фрагментов:"
      ],
      "metadata": {
        "id": "63vS9eeo6YhC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "qn-JhvWDySXM",
        "outputId": "21d2f3ad-b675-4abe-8ea4-bca5f1d87cf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it ' s not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В этом примере вы можете увидеть два перекрывающихся обзора фильмов, один о фильме о старшей школе, а другой о бездомности. Давайте также посмотрим, как выглядят метки для моделирования замаскированного языка:"
      ],
      "metadata": {
        "id": "-CwtDUTG6rG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(lm_datasets[\"train\"][1][\"labels\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "vwIkpuyF65Gz",
        "outputId": "d476d971-8f33-43fd-a287-ea91f5c2baee"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it ' s not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как и ожидалось от нашей функции group_texts() выше, это выглядит идентично декодированным input_ids — _но как тогда наша модель может чему-то научиться?_ Мы упускаем ключевой шаг: вставку токенов [MASK] в случайные позиции во входных данных! Давайте посмотрим, как мы можем сделать это на лету во время тонкой настройки с помощью специального сортировщика данных."
      ],
      "metadata": {
        "id": "o-7-ALwE7EJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Тонкая настройка DistilBERT с помощью API Trainer  "
      ],
      "metadata": {
        "id": "ZWc_9aIC7Sm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### text"
      ],
      "metadata": {
        "id": "rtzX0g4k7mi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тонкая настройка маскированной языковой модели почти идентична тонкой настройке модели классификации последовательностей. Единственное отличие состоит в том, что нам нужен специальный сортировщик данных, который может случайным образом маскировать некоторые токены в каждой партии текстов. К счастью, 🤗 Transformers поставляется с выделенным `DataCollatorForLanguageModeling` именно для этой задачи. Нам просто нужно передать ему токенизатор и аргумент mlm_probability, который указывает, какую долю токенов следует маскировать. Мы выберем 15%, что является суммой, используемой для BERT, и распространенным выбором в литературе:"
      ],
      "metadata": {
        "id": "P1I9Djsy7pf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### code"
      ],
      "metadata": {
        "id": "T0kLjzyc-lGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='yellow'>Важно!, мы имеем датасет с текстами оценок фильмов и мы хотим исходную предобученную модель, обученную на Википедии, использовать для прогнозов [MASK] (т.е. под специфическую задачу, НЕ выделение постов в определенные категории, а заполнять маску!), и при этом для заполнения маски под специфичную категорию постов, связанных с оценкой фильмов. Именно под эту задачу мы производим тонкую настройку исходной модели.</font>"
      ],
      "metadata": {
        "id": "mVFbBJYv8m3A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "c8xWY9PvySXM"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Who9JtsxySXM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff48944a-e7b3-4495-f2c3-bdf96da2ba51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "'>>> [CLS] i rented i am curious - [MASK] [MASK] my video store because of all the controversy that surrounded it when itᄏ first released in 1967. i also heard that at [MASK] it was seized by u. s. customs if it ever tried to enter this country complete therefore being [MASK] fan of films considered \" controversial \" i really had to see this for [MASK]. < [MASK] / > < br / > overseas plot is centered around a young swedish drama student named lena who wants to learn everything she [MASK] about [MASK]. in [MASK] she [MASK] [MASK] focus her attentions feedback making some sort [MASK] documentary inorganic what the average swede thought about [MASK] political issues such'\n",
            "\n",
            "'>>> as the vietnam war and race [MASK] in the united states. [MASK] between asking politicians and ordinary [MASK]izens of stockholm about their opinions on politics, she has [MASK] with [MASK] drama teacher, classmates [MASK] and married men. < br / > [MASK] br / > what kills [MASK] about i [MASK] curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are [MASK] and far between, even then [MASK] ' s not shot like officer cheaply made porno [MASK] [MASK] my country [MASK] [MASK] find it shocking [MASK] in reality [MASK] and nudity [MASK] a major staple in swedish cinema [MASK] even ing holding bergman,'\n"
          ]
        }
      ],
      "source": [
        "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
        "for sample in samples:\n",
        "    _ = sample.pop(\"word_ids\")\n",
        "\n",
        "for chunk in data_collator(samples)[\"input_ids\"]:\n",
        "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### text"
      ],
      "metadata": {
        "id": "C2UQv0fp_Dgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично, сработало! Мы видим, что токен [MASK] был случайным образом вставлен в разные места нашего текста. Это будут токены, которые наша модель должна будет предсказать во время обучения — и прелесть сортировщика данных в том, что он будет рандомизировать вставку [MASK] с каждой партией!\n",
        "\n",
        "✏️ Попробуйте! Запустите фрагмент кода выше несколько раз, чтобы увидеть, как случайная маскировка происходит прямо у вас на глазах! Также замените метод tokenizer.decode() на tokenizer.convert_ids_to_tokens(), чтобы увидеть, что иногда маскируется только один токен из заданного слова, а не другие.\n",
        "\n",
        "Одним из побочных эффектов случайной маскировки является то, что наши метрики оценки не будут детерминированными при использовании Trainer, поскольку мы используем один и тот же сортировщик данных для обучающего и тестового наборов. Позже, когда мы рассмотрим тонкую настройку с помощью 🤗 Accelerate, мы увидим, как мы можем использовать гибкость пользовательского цикла оценки, чтобы заморозить случайность.\n",
        "\n",
        "При обучении моделей для моделирования языка с масками можно использовать один из методов — маскировать целые слова вместе, а не только отдельные токены. Этот подход называется маскировкой целых слов. Если мы хотим использовать маскировку целых слов, нам нужно будет самостоятельно построить сортировщик данных. Коллектор данных — это просто функция, которая берет список образцов и преобразует их в пакет, так что давайте сделаем это сейчас! Мы будем использовать идентификаторы слов, вычисленные ранее, чтобы создать карту между индексами слов и соответствующими токенами, затем случайным образом решим, какие слова маскировать, и применим эту маску к входным данным. Обратите внимание, что все метки равны -100, за исключением тех, которые соответствуют словам маски."
      ],
      "metadata": {
        "id": "1QN-1SpB_Hb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### code"
      ],
      "metadata": {
        "id": "rx-d4T9G_83y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "cgcD5PwNySXM"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "from transformers import default_data_collator\n",
        "\n",
        "wwm_probability = 0.2\n",
        "\n",
        "\n",
        "def whole_word_masking_data_collator(features):\n",
        "    for feature in features:\n",
        "        word_ids = feature.pop(\"word_ids\")\n",
        "\n",
        "        # Create a map between words and corresponding token indices\n",
        "        mapping = collections.defaultdict(list)\n",
        "        current_word_index = -1\n",
        "        current_word = None\n",
        "        for idx, word_id in enumerate(word_ids):\n",
        "            if word_id is not None:\n",
        "                if word_id != current_word:\n",
        "                    current_word = word_id\n",
        "                    current_word_index += 1\n",
        "                mapping[current_word_index].append(idx)\n",
        "\n",
        "        # Randomly mask words\n",
        "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
        "        input_ids = feature[\"input_ids\"]\n",
        "        labels = feature[\"labels\"]\n",
        "        new_labels = [-100] * len(labels)\n",
        "        for word_id in np.where(mask)[0]:\n",
        "            word_id = word_id.item()\n",
        "            for idx in mapping[word_id]:\n",
        "                new_labels[idx] = labels[idx]\n",
        "                input_ids[idx] = tokenizer.mask_token_id\n",
        "        feature[\"labels\"] = new_labels\n",
        "\n",
        "    return default_data_collator(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее мы можем опробовать это на тех же образцах, что и раньше:"
      ],
      "metadata": {
        "id": "z63r9C_bAGYA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "SfV1YNgVySXM",
        "outputId": "db526f50-0802-4ca7-8420-cb4081d7a36f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "'>>> [CLS] i rented i am curious - yellow from my video store because of all the controversy that surrounded [MASK] [MASK] it [MASK] first released in 1967. i also [MASK] [MASK] at first it was [MASK] [MASK] u [MASK] s. customs if [MASK] ever [MASK] to enter this country, [MASK] being [MASK] fan of films considered \" controversial \" i really had [MASK] see this for myself [MASK] < br / [MASK] < [MASK] / > the plot is centered around a young swedish drama student [MASK] [MASK] who wants [MASK] learn everything she can about life. [MASK] particular she wants to focus her attentions to making [MASK] sort of documentary [MASK] what the average swede [MASK] about certain political issues such'\n",
            "\n",
            "'>>> ['[CLS]', 'i', 'rented', 'i', 'am', 'curious', '-', 'yellow', 'from', 'my', 'video', 'store', 'because', 'of', 'all', 'the', 'controversy', 'that', 'surrounded', '[MASK]', '[MASK]', 'it', '[MASK]', 'first', 'released', 'in', '1967', '.', 'i', 'also', '[MASK]', '[MASK]', 'at', 'first', 'it', 'was', '[MASK]', '[MASK]', 'u', '[MASK]', 's', '.', 'customs', 'if', '[MASK]', 'ever', '[MASK]', 'to', 'enter', 'this', 'country', ',', '[MASK]', 'being', '[MASK]', 'fan', 'of', 'films', 'considered', '\"', 'controversial', '\"', 'i', 'really', 'had', '[MASK]', 'see', 'this', 'for', 'myself', '[MASK]', '<', 'br', '/', '[MASK]', '<', '[MASK]', '/', '>', 'the', 'plot', 'is', 'centered', 'around', 'a', 'young', 'swedish', 'drama', 'student', '[MASK]', '[MASK]', 'who', 'wants', '[MASK]', 'learn', 'everything', 'she', 'can', 'about', 'life', '.', '[MASK]', 'particular', 'she', 'wants', 'to', 'focus', 'her', 'attention', '##s', 'to', 'making', '[MASK]', 'sort', 'of', 'documentary', '[MASK]', 'what', 'the', 'average', 'sw', '##ede', '[MASK]', 'about', 'certain', 'political', 'issues', 'such']'\n",
            "\n",
            "'>>> as the [MASK] war and race issues in the united states. [MASK] between asking politicians and ordinary denizens [MASK] stockholm about their [MASK] [MASK] politics, [MASK] has sex with her drama teacher, [MASK], and married men. < br / > < br / [MASK] what kills me about i [MASK] curious - yellow is that 40 years ago, this was considered [MASK] [MASK] really [MASK] the sex [MASK] nudity scenes are few and far between [MASK] even then it [MASK] s not shot like some cheaply made porno. while my [MASK] [MASK] mind find it shocking, in reality sex [MASK] nudity are a major [MASK] in swedish cinema. even ingmar bergman [MASK]'\n",
            "\n",
            "'>>> ['as', 'the', '[MASK]', 'war', 'and', 'race', 'issues', 'in', 'the', 'united', 'states', '.', '[MASK]', 'between', 'asking', 'politicians', 'and', 'ordinary', 'den', '##ize', '##ns', '[MASK]', 'stockholm', 'about', 'their', '[MASK]', '[MASK]', 'politics', ',', '[MASK]', 'has', 'sex', 'with', 'her', 'drama', 'teacher', ',', '[MASK]', ',', 'and', 'married', 'men', '.', '<', 'br', '/', '>', '<', 'br', '/', '[MASK]', 'what', 'kills', 'me', 'about', 'i', '[MASK]', 'curious', '-', 'yellow', 'is', 'that', '40', 'years', 'ago', ',', 'this', 'was', 'considered', '[MASK]', '[MASK]', 'really', '[MASK]', 'the', 'sex', '[MASK]', 'nu', '##dity', 'scenes', 'are', 'few', 'and', 'far', 'between', '[MASK]', 'even', 'then', 'it', '[MASK]', 's', 'not', 'shot', 'like', 'some', 'cheap', '##ly', 'made', 'porn', '##o', '.', 'while', 'my', '[MASK]', '[MASK]', 'mind', 'find', 'it', 'shocking', ',', 'in', 'reality', 'sex', '[MASK]', 'nu', '##dity', 'are', 'a', 'major', '[MASK]', 'in', 'swedish', 'cinema', '.', 'even', 'ing', '##mar', 'bergman', '[MASK]']'\n"
          ]
        }
      ],
      "source": [
        "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
        "batch = whole_word_masking_data_collator(samples)\n",
        "\n",
        "for chunk in batch[\"input_ids\"]:\n",
        "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")\n",
        "    print(f\"\\n'>>> {tokenizer.convert_ids_to_tokens(chunk)}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✏️ Попробуйте! Запустите фрагмент кода выше несколько раз, чтобы увидеть, как случайная маскировка происходит прямо у вас на глазах! Также замените метод tokenizer.decode() на tokenizer.convert_ids_to_tokens(), чтобы увидеть, что токены из заданного слова всегда маскируются вместе.\n",
        "\n",
        "Теперь, когда у нас есть два сортировщика данных, остальные шаги тонкой настройки стандартны. Обучение может занять некоторое время в Google Colab, если вам не повезет получить мифический графический процессор P100 😭, поэтому сначала мы уменьшим размер обучающего набора до нескольких тысяч примеров. Не волнуйтесь, мы все равно получим довольно приличную языковую модель! Быстрый способ уменьшить выборку набора данных в 🤗 Datasets — это функция Dataset.train_test_split(), которую мы видели в Главе 5:"
      ],
      "metadata": {
        "id": "6wzPGYKGAUL7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "5sgg_WfgySXN",
        "outputId": "d2ccdbaf-0c7a-420f-9f00-6c54d2e36588",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "train_size = 10_000\n",
        "test_size = int(0.1 * train_size)\n",
        "\n",
        "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
        "    train_size=train_size, test_size=test_size, seed=42\n",
        ")\n",
        "downsampled_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cm-8ZMaNySXN"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Укажем аргументы для Trainer:"
      ],
      "metadata": {
        "id": "cwl-1lSnBg3v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "OCrzHhPLySXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ec785ee-1606-4e6e-b8e2-a8009d0d9a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 64\n",
        "# Show the training loss with every epoch\n",
        "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    fp16=True,\n",
        "    logging_steps=logging_steps,\n",
        "    output_dir=f\"{model_name}-finetuned-imdb\",\n",
        "    report_to=\"none\"  # Отключает wandb\n",
        "    # overwrite_output_dir=True,\n",
        "    # push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здесь мы подправили несколько параметров по умолчанию, включая logging_steps, чтобы гарантировать отслеживание потерь обучения с каждой эпохой. Мы также использовали fp16=True, чтобы включить обучение со смешанной точностью, что дает нам еще один прирост скорости. По умолчанию Trainer удалит все столбцы, которые не являются частью метода forward() модели.  \n",
        "__Это означает, что если вы используете сортировщик с маскировкой целых слов, вам также нужно будет установить `remove_unused_columns=False`, чтобы гарантировать, что мы не потеряем столбец word_ids во время обучения.__\n",
        "\n",
        "Обратите внимание, что вы можете указать имя репозитория, в который вы хотите отправить данные, с помощью аргумента hub_model_id (в частности, вам придется использовать этот аргумент для отправки в организацию). Например, когда мы отправили модель в организацию huggingface-course, мы добавили hub_model_id=\"huggingface-course/distilbert-finetuned-imdb\" в TrainingArguments. По умолчанию используемый репозиторий будет находиться в вашем пространстве имен и назван в честь выходного каталога, который вы установили, поэтому в нашем случае это будет \"lewtun/distilbert-finetuned-imdb\".\n",
        "\n",
        "Теперь у нас есть все ингредиенты для создания экземпляра Trainer. Здесь мы просто используем стандартный data_collator, но вы можете попробовать целословный маскирующий collator и сравнить результаты в качестве упражнения:"
      ],
      "metadata": {
        "id": "LNbxmHdUB0br"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "vxO_Q6m4ySXN"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=downsampled_dataset[\"train\"],\n",
        "    eval_dataset=downsampled_dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь мы готовы запустить trainer.train(), но перед этим давайте кратко рассмотрим перплексию, которая является распространенной метрикой для оценки производительности языковых моделей."
      ],
      "metadata": {
        "id": "Pegymv8VCkYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В отличие от других задач, таких как классификация текста или ответы на вопросы, где нам дается помеченный корпус для обучения, при языковом моделировании у нас нет никаких явных меток. Так как же нам определить, что делает языковую модель хорошей? Как и в случае с функцией автозамены в вашем телефоне, хорошая языковая модель — это та, которая назначает высокие вероятности предложениям, которые грамматически правильны, и низкие вероятности бессмысленным предложениям. Чтобы дать вам лучшее представление о том, как это выглядит, вы можете найти целые наборы «неудач автозамены» в Интернете, где модель в телефоне человека выдала некоторые довольно забавные (и часто неуместные) завершения!\n",
        "\n",
        "Предполагая, что наш тестовый набор состоит в основном из предложений, которые грамматически правильны, тогда один из способов измерить качество нашей языковой модели — это вычислить вероятности, которые она назначает следующему слову во всех предложениях тестового набора. Высокая вероятность указывает на то, что модель не «удивлена» и не «озадачена» невидимыми примерами, и предполагает, что она усвоила основные закономерности грамматики языка. Существуют различные математические определения озадаченности, но то, которое мы будем использовать, определяет ее как экспоненту потери кросс-энтропии. Таким образом, мы можем вычислить озадаченность нашей предварительно обученной модели, используя функцию Trainer.evaluate() для вычисления потери кросс-энтропии на тестовом наборе, а затем взяв экспоненту результата:"
      ],
      "metadata": {
        "id": "LTlyH4S2GJc8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "iL0KBFqKySXN",
        "outputId": "4818ae97-01c6-444f-c946-63479a08812f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Perplexity: 21.94\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Более низкий показатель perplexity означает лучшую языковую модель, и мы можем видеть здесь, что наша начальная модель имеет довольно большое значение. Давайте посмотрим, сможем ли мы понизить его с помощью тонкой настройки! Для этого мы сначала запускаем цикл обучения:"
      ],
      "metadata": {
        "id": "TyLn5g_bGS5X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "n1zi3svZySXN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "94af5346-fbab-4971-9d5d-0f2ce0fafba4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [471/471 02:33, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Model Preparation Time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.683800</td>\n",
              "      <td>2.509436</td>\n",
              "      <td>0.001700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.587800</td>\n",
              "      <td>2.450192</td>\n",
              "      <td>0.001700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.527900</td>\n",
              "      <td>2.481931</td>\n",
              "      <td>0.001700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 00:52]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=471, training_loss=2.599245999775621, metrics={'train_runtime': 153.9359, 'train_samples_per_second': 194.886, 'train_steps_per_second': 3.06, 'total_flos': 994208670720000.0, 'train_loss': 2.599245999775621, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "и затем вычислить результирующую сложность на тестовом наборе, как и раньше:"
      ],
      "metadata": {
        "id": "PrWCYdsHF3Wl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "evOK1J3PySXN",
        "outputId": "96ba63c5-29e4-4c84-fb37-fbcd2c6b3eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Perplexity: 12.05\n"
          ]
        }
      ],
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично — это значительное снижение недоумения, которое говорит нам, что модель узнала что-то о области обзоров фильмов!"
      ],
      "metadata": {
        "id": "2dNQZp3fHCTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Сохранение модели"
      ],
      "metadata": {
        "id": "xeBh11kKG9Kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGRfSSowHfs1",
        "outputId": "f71eb248-ff98-4ed0-dc7e-89d0bc870e4c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/My Drive/Hugging_face/distilbert-base-uncased_fine_tuning\""
      ],
      "metadata": {
        "id": "Eruoq-5vHr_Q"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.push_to_hub()\n",
        "output_dir = file_path  # Укажите путь для сохранения\n",
        "trainer.save_model(output_dir)  # Сохранит модель и конфиг\n",
        "tokenizer.save_pretrained(output_dir)  # Сохранит токенизатор"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZA49uq-HQJf",
        "outputId": "1e73c387-dbc1-4ee3-b1d0-a8abf4e97d1f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/Hugging_face/distilbert-base-uncased_fine_tuning/tokenizer_config.json',\n",
              " '/content/drive/My Drive/Hugging_face/distilbert-base-uncased_fine_tuning/special_tokens_map.json',\n",
              " '/content/drive/My Drive/Hugging_face/distilbert-base-uncased_fine_tuning/vocab.txt',\n",
              " '/content/drive/My Drive/Hugging_face/distilbert-base-uncased_fine_tuning/added_tokens.json',\n",
              " '/content/drive/My Drive/Hugging_face/distilbert-base-uncased_fine_tuning/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы загрузить модель:  \n",
        "```\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "trainer.model = AutoModelForSequenceClassification.from_pretrained(file_path)\n",
        "```\n"
      ],
      "metadata": {
        "id": "SDN-ow7fIlO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### code"
      ],
      "metadata": {
        "id": "zCLb3ib1MtI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✏️ Ваша очередь! Запустите обучение выше, изменив сортировщик данных на сортировщик с маскировкой целых слов. Вы получаете лучшие результаты?"
      ],
      "metadata": {
        "id": "JDIyeTqzGkkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____________________________\n",
        "_____________________________"
      ],
      "metadata": {
        "id": "RxBx0nTTRQo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args_2 = TrainingArguments(\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    fp16=True,\n",
        "    logging_steps=logging_steps,\n",
        "    output_dir=f\"{model_name}-finetuned-imdb\",\n",
        "    report_to=\"none\",  # Отключает wandb\n",
        "    remove_unused_columns=False, # Изменения\n",
        "    # overwrite_output_dir=True,\n",
        "    # push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG3FBdeuN4-i",
        "outputId": "f98f5e73-383e-4898-dcbe-e156ec680766"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_2 = Trainer(\n",
        "    model=model,\n",
        "    args=training_args_2,\n",
        "    train_dataset=downsampled_dataset[\"train\"],\n",
        "    eval_dataset=downsampled_dataset[\"test\"],\n",
        "    data_collator=whole_word_masking_data_collator, # Изменения\n",
        "    processing_class=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "x2Xl0prDMMb-"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_2.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "MzUGLecqNNoZ",
        "outputId": "4272d697-cefa-46db-d357-3eadd6b34ab2"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [471/471 02:36, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.398800</td>\n",
              "      <td>3.291717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.325600</td>\n",
              "      <td>3.295487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.290800</td>\n",
              "      <td>3.252086</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=471, training_loss=3.3389588738702667, metrics={'train_runtime': 156.6676, 'train_samples_per_second': 191.488, 'train_steps_per_second': 3.006, 'total_flos': 994208670720000.0, 'train_loss': 3.3389588738702667, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results_2 = trainer_2.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "781mhYqWPLrx",
        "outputId": "9f483935-810c-438a-d326-866c28803376"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Perplexity: 12.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "То есть, хотя мы и использовали \"сортировщик\" на целые слова, результат не улучшился. Попробуем увеличить обучающий набор.  "
      ],
      "metadata": {
        "id": "GdsIFtH6PS0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size_2 = 20_000\n",
        "test_size_2 = int(0.1 * train_size)\n",
        "\n",
        "downsampled_dataset_2 = lm_datasets[\"train\"].train_test_split(\n",
        "    train_size=train_size_2, test_size=test_size_2, seed=42\n",
        ")\n",
        "downsampled_dataset_2"
      ],
      "metadata": {
        "id": "CSuQmPscPkuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 64\n",
        "# Show the training loss with every epoch\n",
        "logging_steps = len(downsampled_dataset_2[\"train\"]) // batch_size\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "training_args_3 = TrainingArguments(\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    fp16=True,\n",
        "    logging_steps=logging_steps,\n",
        "    output_dir=f\"{model_name}-finetuned-imdb\",\n",
        "    report_to=\"none\",  # Отключает wandb\n",
        "    remove_unused_columns=False, # Изменения\n",
        "    # overwrite_output_dir=True,\n",
        "    # push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdqUlanqQAYt",
        "outputId": "497480ac-ba0d-4a10-b31b-e8ce2a6c4ef8"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_3 = Trainer(\n",
        "    model=model,\n",
        "    args=training_args_3,\n",
        "    train_dataset=downsampled_dataset_2[\"train\"],\n",
        "    eval_dataset=downsampled_dataset_2[\"test\"],\n",
        "    data_collator=whole_word_masking_data_collator, # Изменения\n",
        "    processing_class=tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "sYWpVaDMQUGr"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_3.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "A-fYCV-TQkdv",
        "outputId": "3b2907dc-5ab7-4a3d-c6f5-776a3d3a7ae5"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='939' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [939/939 05:26, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.312400</td>\n",
              "      <td>3.200478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.250400</td>\n",
              "      <td>3.190410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.222800</td>\n",
              "      <td>3.160626</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=939, training_loss=3.261841346407597, metrics={'train_runtime': 326.5069, 'train_samples_per_second': 183.763, 'train_steps_per_second': 2.876, 'total_flos': 1988417341440000.0, 'train_loss': 3.261841346407597, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results_3 = trainer_3.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "bPIJeLnnQxHP",
        "outputId": "e954c380-47a7-4012-8173-f98900ae6567"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16/16 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Perplexity: 12.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Не улучшается ..........."
      ],
      "metadata": {
        "id": "uMx-vMlLSHw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В нашем случае нам не нужно было делать ничего особенного с циклом обучения, но в некоторых случаях вам может потребоваться реализовать некоторую пользовательскую логику. Для этих приложений вы можете использовать 🤗 Accelerate — давайте посмотрим!"
      ],
      "metadata": {
        "id": "AZSSFKIXHNbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Тонкая настройка DistilBERT с помощью 🤗 Accelerate"
      ],
      "metadata": {
        "id": "BVCyLevtQ9CH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как мы видели с Trainer, тонкая настройка маскированной языковой модели очень похожа на пример классификации текста из главы 3. Фактически, единственная тонкость — это использование специального сортировщика данных, и мы уже рассмотрели это ранее в этом разделе!\n",
        "\n",
        "Однако мы увидели, что DataCollatorForLanguageModeling также применяет случайную маскировку при каждой оценке, поэтому мы увидим некоторые колебания в наших оценках озадаченности при каждом запуске обучения. Один из способов устранить этот источник случайности — применить маскировку один раз ко всему тестовому набору, а затем использовать сортировщик данных по умолчанию в 🤗 Transformers для сбора пакетов во время оценки. Чтобы увидеть, как это работает, давайте реализуем простую функцию, которая применяет маскировку к пакету, аналогично нашей первой встрече с DataCollatorForLanguageModeling:"
      ],
      "metadata": {
        "id": "nQZ_TEYsRCQk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "mga2cFmOySXO"
      },
      "outputs": [],
      "source": [
        "def insert_random_mask(batch):\n",
        "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
        "    masked_inputs = data_collator(features)\n",
        "    # Create a new \"masked\" column for each column in the dataset\n",
        "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее мы применим эту функцию к нашему тестовому набору и удалим немаскированные столбцы, чтобы заменить их замаскированными. Вы можете использовать маскировку целых слов, заменив data_collator выше на соответствующий, в этом случае вам следует удалить первую строку здесь:"
      ],
      "metadata": {
        "id": "QqVNrc5YR0qW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "szRST8gJySXO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "bed3783d049f461eb9effe27fd654ff1",
            "9b35b14ca4bc44fd8e684dbb7308eb8a",
            "cc9e2371dc6d4e29859d101e02481b49",
            "cd467d73cf7e46deb58bf1f56c4372dc",
            "59f1f961e13b449fbb6261cc3bf333c5",
            "3da2373c627845a3bee68b0fd637577e",
            "f4887531eb444fd89ad7e06197550269",
            "d7ca26db9ceb45e5a7efcf6e2555bca7",
            "5ad0eb1815a44ce9a5993c97b52f9183",
            "b0cfaf2b52074e6381de9e244c155bd2",
            "2d183cc7eff347eb8474e9156bbfc1aa"
          ]
        },
        "outputId": "df5e807d-82f8-46eb-c60b-27c88f96e9f0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bed3783d049f461eb9effe27fd654ff1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "downsampled_dataset = downsampled_dataset.remove_columns([\"word_ids\"])\n",
        "eval_dataset = downsampled_dataset[\"test\"].map(\n",
        "    insert_random_mask,\n",
        "    batched=True,\n",
        "    remove_columns=downsampled_dataset[\"test\"].column_names,\n",
        ")\n",
        "eval_dataset = eval_dataset.rename_columns(\n",
        "    {\n",
        "        \"masked_input_ids\": \"input_ids\",\n",
        "        \"masked_attention_mask\": \"attention_mask\",\n",
        "        \"masked_labels\": \"labels\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Затем мы можем настроить загрузчики данных как обычно, но для оценочного набора мы будем использовать default_data_collator из 🤗 Transformers:"
      ],
      "metadata": {
        "id": "SqrylooqSlwq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "5Aws6YhoySXO"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "\n",
        "batch_size = 64\n",
        "train_dataloader = DataLoader(\n",
        "    downsampled_dataset[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Форма здесь, мы следуем стандартным шагам с 🤗 Accelerate. Первым делом нужно загрузить свежую версию предварительно обученной модели:"
      ],
      "metadata": {
        "id": "Zqv3VG1LSyl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "C4nevjJHS5gH"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Затем нам нужно указать оптимизатор, будем использовать стандартный AdamW:"
      ],
      "metadata": {
        "id": "KFxkC22NS_yo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "cUTf4I1hySXO"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "С помощью этих объектов мы теперь можем подготовить все для обучения с помощью объекта Accelerator:"
      ],
      "metadata": {
        "id": "H4nuh9rXTFzr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "hRxNI4VbySXR"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator()\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь, когда наша модель, оптимизатор и загрузчики данных настроены, мы можем указать планировщик скорости обучения следующим образом:"
      ],
      "metadata": {
        "id": "ylS17ngGTMeA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "995sXNZ1ySXR"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = 3\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdtMG9FuySXR",
        "outputId": "a3b22eff-5cb1-4bc8-d93a-29f27dd22129"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'lewtun/distilbert-base-uncased-finetuned-imdb-accelerate'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from huggingface_hub import get_full_repo_name\n",
        "\n",
        "# model_name = \"distilbert-base-uncased-finetuned-imdb-accelerate\"\n",
        "# repo_name = get_full_repo_name(model_name)\n",
        "# repo_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJF4QeVxySXS"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import Repository\n",
        "\n",
        "# output_dir = model_name\n",
        "# repo = Repository(output_dir, clone_from=repo_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подготовим путь для записи обученной модели"
      ],
      "metadata": {
        "id": "IDEQd4IDTu8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_2 = \"/content/drive/My Drive/Hugging_face/distilbert-base-uncased_accelerate\""
      ],
      "metadata": {
        "id": "f4IyzwlOUB7y"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "После этого останется только написать полный цикл обучения и оценки:"
      ],
      "metadata": {
        "id": "E2fRl4daUO9e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "5Qs5kkZ1ySXS",
        "outputId": "9de08be4-de56-41c9-f8a7-6790210d120a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "4961facaa6c24350b7c134a6f119f6de",
            "0ac8b0fdd57e49bda1571afbdfd1d649",
            "ad2c054443324acf8a90616b5899d468",
            "0a90d44c799d4e42894d3199e87475ad",
            "a9ac08c199db4f888d4ff1f3f0af22f5",
            "929103abd913483b81c7a00c71d53c09",
            "5277ee1d257b4029a78f4cc0ed4aa660",
            "85d85a81cd0c4202961b0b6c6bbe9634",
            "335fdf1f487a4edcab5f29567dd97016",
            "ef3ccd56a0fc40fdad450ef1f6d1a306",
            "0c45d2c2e7a74c66886beb72b55e4fb6"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/471 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4961facaa6c24350b7c134a6f119f6de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Epoch 0: Perplexity: 10.88425165507281\n",
            ">>> Epoch 1: Perplexity: 10.670057089700505\n",
            ">>> Epoch 2: Perplexity: 10.670057089700505\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import math\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(num_train_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
        "\n",
        "    losses = torch.cat(losses)\n",
        "    losses = losses[: len(eval_dataset)]\n",
        "    try:\n",
        "        perplexity = math.exp(torch.mean(losses))\n",
        "    except OverflowError:\n",
        "        perplexity = float(\"inf\")\n",
        "\n",
        "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
        "\n",
        "    # Save and upload\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    unwrapped_model.save_pretrained(file_path_2, save_function=accelerator.save)\n",
        "    if accelerator.is_main_process:\n",
        "        tokenizer.save_pretrained(file_path_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здорово, нам удалось оценить сложность в каждой эпохе и гарантировать воспроизводимость множественных тренировочных запусков!"
      ],
      "metadata": {
        "id": "L6CMzOcSU9YO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Использование нашей тонко настроенной модели"
      ],
      "metadata": {
        "id": "eE4XkXgZVEty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим модель с помощью конвейера fill-mask:"
      ],
      "metadata": {
        "id": "yhSb4ow6VHeR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "DDOq7647ySXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2023500d-a0ba-4b21-b927-d2036ba71982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "mask_filler = pipeline(\"fill-mask\", model=file_path_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "aKt9ErrJySXS",
        "outputId": "edd0cca8-dfbe-4548-d5a9-c9fd7e7f62c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> this is a great film.\n",
            ">>> this is a great movie.\n",
            ">>> this is a great idea.\n",
            ">>> this is a great one.\n",
            ">>> this is a great story.\n"
          ]
        }
      ],
      "source": [
        "# text = \"This is a great [MASK].\"\n",
        "preds = mask_filler(text)\n",
        "\n",
        "for pred in preds:\n",
        "    print(f\">>> {pred['sequence']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model has clearly adapted its weights to predict words that are more strongly associated with movies!"
      ],
      "metadata": {
        "id": "co_BrLyTWCH4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "Ff49JeLDC6Xh",
        "er8L7CXHCKUr",
        "xXYZMtlxNVGO",
        "xWFgm0H6GntK",
        "rhHXd8qygyfC",
        "bTpkO_XHg2_V",
        "86SNeNSLiVDH",
        "Ow6n4z2Wikup",
        "8_5c_KhtzYhd",
        "dDCglQM60i8B",
        "0dyPbAbd15Zf",
        "sjk_LATI2M7P",
        "ySBhUg2A4Uo7",
        "XnKsktyC4bGZ",
        "rtzX0g4k7mi9",
        "C2UQv0fp_Dgk"
      ]
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ae022ce575114d1e9c5c6035af855083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_7aed193c44e547ce8206fe6ebde2ad9d"
          }
        },
        "f17e56a195d94676bd7fd749757d0216": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d63845b707fd4e1dabdf03b4bc0202ff",
            "placeholder": "​",
            "style": "IPY_MODEL_226d54e516fb445287309a3b6757e79c",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "65ed6df8ca21481599264c5aa83022d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_e6f3f6f316e0465faca33af8846114ad",
            "placeholder": "​",
            "style": "IPY_MODEL_f0cbdf558bb842c998c770298c292ef0",
            "value": ""
          }
        },
        "d86de6945ab34774bc6da09983f62c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_fe3b927fb7fe4493a5411d4eb75ddb5d",
            "style": "IPY_MODEL_340bde4ab02f4c148006251ec1c703ae",
            "value": true
          }
        },
        "dbf119930e2f41208c63638eee7583db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_4fec027ef9b4476f83783524f36e0787",
            "style": "IPY_MODEL_c85b1d62c335425f914b23f228371e42",
            "tooltip": ""
          }
        },
        "12c1754d08da4a52b3b6149920c8a378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a57c624fc36749d980b765ede06d34af",
            "placeholder": "​",
            "style": "IPY_MODEL_df331bfbe9b842faa0cf6a0a7e9124c0",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "7aed193c44e547ce8206fe6ebde2ad9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "d63845b707fd4e1dabdf03b4bc0202ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "226d54e516fb445287309a3b6757e79c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6f3f6f316e0465faca33af8846114ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0cbdf558bb842c998c770298c292ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe3b927fb7fe4493a5411d4eb75ddb5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "340bde4ab02f4c148006251ec1c703ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fec027ef9b4476f83783524f36e0787": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c85b1d62c335425f914b23f228371e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "a57c624fc36749d980b765ede06d34af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df331bfbe9b842faa0cf6a0a7e9124c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "deb05a588b62435d8a78b851d033fb4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb7eb687386240f9b36422629e03e8b6",
            "placeholder": "​",
            "style": "IPY_MODEL_3e4b4be8d8834dcc94b983d591f8c691",
            "value": "Connecting..."
          }
        },
        "bb7eb687386240f9b36422629e03e8b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e4b4be8d8834dcc94b983d591f8c691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bed3783d049f461eb9effe27fd654ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b35b14ca4bc44fd8e684dbb7308eb8a",
              "IPY_MODEL_cc9e2371dc6d4e29859d101e02481b49",
              "IPY_MODEL_cd467d73cf7e46deb58bf1f56c4372dc"
            ],
            "layout": "IPY_MODEL_59f1f961e13b449fbb6261cc3bf333c5"
          }
        },
        "9b35b14ca4bc44fd8e684dbb7308eb8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3da2373c627845a3bee68b0fd637577e",
            "placeholder": "​",
            "style": "IPY_MODEL_f4887531eb444fd89ad7e06197550269",
            "value": "Map: 100%"
          }
        },
        "cc9e2371dc6d4e29859d101e02481b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7ca26db9ceb45e5a7efcf6e2555bca7",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ad0eb1815a44ce9a5993c97b52f9183",
            "value": 1000
          }
        },
        "cd467d73cf7e46deb58bf1f56c4372dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0cfaf2b52074e6381de9e244c155bd2",
            "placeholder": "​",
            "style": "IPY_MODEL_2d183cc7eff347eb8474e9156bbfc1aa",
            "value": " 1000/1000 [00:00&lt;00:00, 3236.84 examples/s]"
          }
        },
        "59f1f961e13b449fbb6261cc3bf333c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3da2373c627845a3bee68b0fd637577e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4887531eb444fd89ad7e06197550269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7ca26db9ceb45e5a7efcf6e2555bca7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ad0eb1815a44ce9a5993c97b52f9183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0cfaf2b52074e6381de9e244c155bd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d183cc7eff347eb8474e9156bbfc1aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4961facaa6c24350b7c134a6f119f6de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ac8b0fdd57e49bda1571afbdfd1d649",
              "IPY_MODEL_ad2c054443324acf8a90616b5899d468",
              "IPY_MODEL_0a90d44c799d4e42894d3199e87475ad"
            ],
            "layout": "IPY_MODEL_a9ac08c199db4f888d4ff1f3f0af22f5"
          }
        },
        "0ac8b0fdd57e49bda1571afbdfd1d649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_929103abd913483b81c7a00c71d53c09",
            "placeholder": "​",
            "style": "IPY_MODEL_5277ee1d257b4029a78f4cc0ed4aa660",
            "value": "100%"
          }
        },
        "ad2c054443324acf8a90616b5899d468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85d85a81cd0c4202961b0b6c6bbe9634",
            "max": 471,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_335fdf1f487a4edcab5f29567dd97016",
            "value": 470
          }
        },
        "0a90d44c799d4e42894d3199e87475ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef3ccd56a0fc40fdad450ef1f6d1a306",
            "placeholder": "​",
            "style": "IPY_MODEL_0c45d2c2e7a74c66886beb72b55e4fb6",
            "value": " 470/471 [02:40&lt;00:00,  3.34it/s]"
          }
        },
        "a9ac08c199db4f888d4ff1f3f0af22f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "929103abd913483b81c7a00c71d53c09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5277ee1d257b4029a78f4cc0ed4aa660": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85d85a81cd0c4202961b0b6c6bbe9634": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "335fdf1f487a4edcab5f29567dd97016": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef3ccd56a0fc40fdad450ef1f6d1a306": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c45d2c2e7a74c66886beb72b55e4fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}