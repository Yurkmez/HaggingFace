{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI8SuiAZmnn5"
      },
      "source": [
        "# Fine-tuning a model with the Trainer API or Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xp3ViAJomnn9"
      },
      "outputs": [],
      "source": [
        "# Install the Transformers, Datasets, and Evaluate libraries to run this notebook.\n",
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<small>Transformers предоставляет класс Trainer, который позволяет выполнить  тонкую настройку предварительно обученной модели на нашем наборе данных.</small>"
      ],
      "metadata": {
        "id": "MFrxT7bdoIL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка данных"
      ],
      "metadata": {
        "id": "bA413B7lp7mC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "SPVeGkpkmnn-"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
      ],
      "metadata": {
        "id": "U7AwfIKvzZgi"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "75895baca542443b953e535da6fba412",
            "0ace11875170457cb6a39a9c501ef154",
            "a7892a95fe604e359d9274b21566a3ef",
            "8ae8c7a157d24af4a9db6a0a5139e1ae",
            "a7b6d0198d6d4ddb92c1d5e633756811",
            "4b456a04d1a546749144b0acfd876cd4",
            "7dcd678364ef4e378d138715933de386",
            "a125ae2bf3f14795a948f592d50c9e52",
            "12cc076a926d4b3992af57cc59682873",
            "cd1099ebe5e14cb8a60ba03423d4fef6",
            "b4bc416f260b47c888dc5b8b6e44ea0f"
          ]
        },
        "id": "kKZme4g3zfA3",
        "outputId": "d2de6504-83ca-47d2-eacb-e9ea39f316b8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/408 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75895baca542443b953e535da6fba412"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = tokenized_datasets[\"train\"][:5]\n",
        "for i in range(5):\n",
        "    print(len(examples[\"input_ids\"][i]))  # 50, 59, 47, 67, 59\n",
        "\n",
        "# или\n",
        "# for example in tokenized_datasets[\"train\"].select(range(5)):\n",
        "#     print(len(example[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "8xGg1AVh36cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "b1c3UyH18ZiI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator\n",
        "# мы его будем использовать ниже, когда будем передавать параметры \"тренеру\""
      ],
      "metadata": {
        "id": "rCbw43BE8cPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение"
      ],
      "metadata": {
        "id": "q1vGNJyNohgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<small>Первый шаг перед тем, как мы сможем определить наш Trainer, — это определить класс TrainingArguments, который будет содержать все гиперпараметры, которые Trainer будет использовать для обучения и оценки. Единственный аргумент, который вам нужно предоставить, — это каталог, в котором будет сохранена обученная модель, а также контрольные точки по пути. Для всего остального вы можете оставить значения по умолчанию, которые должны работать довольно хорошо для базовой тонкой настройки.</small>"
      ],
      "metadata": {
        "id": "4N6YhIiHomHb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "uHWLeG3wmnn-"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "# training_args = TrainingArguments(\"test-trainer\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert-output\",\n",
        "    report_to=\"none\"  # Отключает wandb\n",
        ")\n",
        "# training_args"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<small>Используем `AutoModelForSequenceClassification` класс моделей, указываем к-во классов для предсказания, создаем  экземпляр модели:"
      ],
      "metadata": {
        "id": "aPM1QOacp0fu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2tiEkwUmnn_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<small>созданный экземпляр этой предварительно обученной модели - BERT не был обучен классификации пар предложений, поэтому старая \"голова\" был отброшена, а в новой веса инициализированы случайным образом. В заключение предлагается обучить модель, что мы и собираемся сделать сейчас.  \n",
        "И мы формируем наш trainer, передав ему — модель, гиперпараметры (training_args), наборы данных для обучения и проверки, наш data_collator и наш tokenizer:</small>"
      ],
      "metadata": {
        "id": "BPyYbUVupmqp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "4VT9Noltmnn_"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    # data_collator=data_collator,\n",
        "    processing_class=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<small>Здесь мы передали токенизатор, который мы использовали, когда ранее формировали data_collator (`data_collator = DataCollatorWithPadding(tokenizer=tokenizer`). По умолчанию, Trainer аналогично формирует data_collator, поэтому можно пропустить строку \"data_collator=data_collator\" в этом вызове.   \n",
        "И, чтобы точно настроить модель на нашем наборе данных, осталось вызвать метод train() нашего Trainer:<.small>"
      ],
      "metadata": {
        "id": "7dBSIugfquct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Пооверяем, подключен ли графический ускоритель\n",
        "# Иначе дообучение будет длится несколько часов (с ускорителем - несколько минут)\n",
        "import torch\n",
        "print(\"GPU доступен:\", torch.cuda.is_available())\n",
        "print(\"Название GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Нет GPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb9IEsHx0Gjz",
        "outputId": "fe4225d9-fd4e-41ed-e430-9947818a947a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU доступен: True\n",
            "Название GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "_150BuNymnn_",
        "outputId": "6096c8cd-d92a-400e-d473-41834e7567c2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1377/1377 03:28, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.242400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.114300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1377, training_loss=0.15301497881189152, metrics={'train_runtime': 209.1206, 'train_samples_per_second': 52.62, 'train_steps_per_second': 6.585, 'total_flos': 405114969714960.0, 'train_loss': 0.15301497881189152, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<small>Это запустит тонкую настройку (которая должна занять пару минут на GPU) и сообщит о потере обучения каждые 500 шагов. Однако это не отражает, насколько хорошо (или плохо) работает ваша модель, т.к. мы не указали параметры оценки: Evaluation_strategy на \"steps\" (оценивать каждые eval_steps) или \"epoch\" (оценивать в конце каждой эпохи). И мы не предоставили Тренеру функцию compute_metrics() для расчета метрики во время указанной оценки (иначе оценка просто вывела бы потерю, что не является очень интуитивно понятным числом).   \n",
        "Чтобы получить прогнозы используется команда `trainer.predict()`:</small>"
      ],
      "metadata": {
        "id": "z9asBPDsx_zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "print(predictions.predictions.shape, predictions.label_ids.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "BuDurjBehQqR",
        "outputId": "cf33f1d0-4e58-4d14-fbfc-1d3ada83606d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(408, 2) (408,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions[0][0]) # [-2.6618629  2.3648045]\n",
        "# или\n",
        "# print(predictions.predictions[0]) # [-2.6618629  2.3648045]\n",
        "# так как predictions – это объект, содержащий:\n",
        "# _________predictions.predictions – массив логитов (сырые выходы нейросети).\n",
        "# _________predictions.label_ids – истинные метки (если они есть).\n",
        "# _________predictions.metrics – метрики модели.\n",
        "\n",
        "# print(predictions[1]) # [1 0 0 1 0 ...]\n",
        "# print(predictions[2]) # {'test_loss': 0.686876118183136, 'test_runtime': 2.7329, 'test_samples_per_second': 149.294, 'test_steps_per_second': 18.662}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n0XAbFKhXXc",
        "outputId": "3002cfdf-c640-486d-fb82-39af2323f3c1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.1932015  3.4147985]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<small>Выход метода predict() — это двумерный массив размером 408 x 2 (408 — количество элементов в наборе данных) в форме кортежа с тремя полями: `predicts, label_ids и metrics`, где  \n",
        "`predicts[0]` — содержит логиты для каждого элемента набора данных и чтобы преобразовать их в прогнозы, которые можно сравнить с нашими метками, нужно взять индекс с максимальным значением на второй оси:</small>"
      ],
      "metadata": {
        "id": "HpOQ08vchlVo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "dtozze92mnoA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9ed034-bc35-4713-cb7e-07dcfb7dab1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "import numpy as np\n",
        "preds = np.argmax(predictions.predictions, axis=-1)\n",
        "preds[:5] # array([1, 0, 0, 1, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### __Что здесь происходит__  "
      ],
      "metadata": {
        "id": "o0ZdAjnbrqxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "в `predictions.predictions` мы имеем логиты вида `[-2.6618629  2.3648045]`  \n",
        "где, если 1 первое значение больше второго, то у нас предложения не являются парафразами, если иначе - они парафразы. Функция np.argmax() находит индекс наибольшего значения по указанной оси, то есть, если второе значение больше  \n",
        "'''\n",
        "Индекс            0           1\n",
        "Логит       [-2.6618629  __2.3648045__]\n",
        "'''\n",
        "то `np.argmax()` вернет индекс 1, что означает (в соответствии с определенными в датасете значениями меток: 1 - парафразы, 0 - нет) предложения являются парафразами и наоборот.\n",
        "\n",
        "__По поводу оси (axis=-1).__  \n",
        "<small>Так как `logits` содержит массив логитов размерности:\n",
        "`(batch_size, num_classes)`, где  \n",
        "- `batch_size` – количество примеров в батче,  \n",
        "- `num_classes` – количество классов, для которых модель предсказывает логиты,  \n",
        "то `axis=1 (или axis=-1)` – ищет максимум по классам - для каждого примера найдёт самый вероятный класс (в то время, как `axis=0` – ищет максимум по батчам - вернёт один максимум на каждый класс).</small>\n",
        "\n",
        "Еще пример (несколько классов).\n",
        "```\n",
        "logits = np.array([\n",
        "    [2.3, 1.1, 0.5],  # 1-й пример → 2.3 (индекс 0)\n",
        "    [0.2, 3.4, 2.1],  # 2-й пример → 3.4 (индекс 1)\n",
        "    [1.2, 0.8, 4.0]   # 3-й пример → 4.0 (индекс 2)\n",
        "])\n",
        "\n",
        "preds = np.argmax(logits, axis=-1)\n",
        "print(preds)  # [0 1 2]\n",
        "```\n"
      ],
      "metadata": {
        "id": "q2DgP1wqYVUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Кстати, если вместо предсказанных классов нужны вероятности (например, для анализа уверенности модели), надо использовать softmax:"
      ],
      "metadata": {
        "id": "uNfev2p4eUOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "probs = softmax(predictions.predictions, axis=-1)  # Преобразуем логиты в вероятности\n",
        "print(probs[:5])  # Теперь числа от 0 до 1, сумма по каждому примеру = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9McLYxkeanu",
        "outputId": "48878f76-eabf-4aa1-b609-f601caf59fdf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00651788 0.99348205]\n",
            " [0.9929617  0.00703833]\n",
            " [0.06869959 0.93130034]\n",
            " [0.00665813 0.99334186]\n",
            " [0.99389595 0.00610403]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation (Оценка)"
      ],
      "metadata": {
        "id": "W37gcRylhxI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Теперь можно сравнить эти предсказанные значения с метками.__  \n",
        "Чтобы построить функцию `compute_metric()`, используем метрики из библиотеки 🤗 `Evaluate`.  \n",
        "Загрузим метки из набора данных MRPC с помощью функции `estimate.load()` и выполним сравнение с предсказанными метками с применением метода `compute()`, который выдаёт такие метрики, как `accuracy` (точность) и `F1-score`.  \n",
        "<small>Accuracy – доля правильно предсказанных примеров.\n",
        "F1-score – баланс между точностью (precision) и полнотой (recall).\n",
        "</small>"
      ],
      "metadata": {
        "id": "Q1bdJYoqh2aU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7QHeacwmnoA",
        "outputId": "7f544efa-6b4e-4555-97f0-9f3599cfe636"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.8627450980392157, 'f1': 0.9041095890410958}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "metric.compute(predictions=preds, references=predictions.label_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`evaluate.load(\"glue\", \"mrpc\")` - загружает метрику GLUE (General Language Understanding Evaluation). mrpc (Microsoft Research Paraphrase Corpus) – это задача бинарной классификации: определение, являются ли два предложения перефразами.Выдаёт такие метрики, как accuracy (точность) и F1-score.  \n",
        "Что здесь - `predictions=preds, references=predictions.label_ids`  \n",
        "- `predictions=preds` - в preds нули и единицы, т.е., то что мы предсказали  \n",
        "- `predictions.label_ids` - как было выше отмечено в пояснениях,  \n",
        "\n",
        "\n",
        "<small>__`predictions`__ – это объект, содержащий:\n",
        "- `predictions.predictions` – массив логитов (сырые выходы нейросети),  \n",
        "- `predictions.label_ids` – истинные метки,  \n",
        "- `predictions.metrics` – метрики модели.</small>"
      ],
      "metadata": {
        "id": "vzpoCd1pgNWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Объединив все"
      ],
      "metadata": {
        "id": "gvdURsCKrLIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объединяя все вместе, мы получаем нашу функцию compute_metrics():"
      ],
      "metadata": {
        "id": "qu1uQ3jy4EzH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "PxRgR_LpmnoB"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Разбор `compute_metrics=compute_metrics`  "
      ],
      "metadata": {
        "id": "TUSxJE3Br8CF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer сам её вызовет во время оценки (evaluation), передавая нужные данные.  \n",
        "Что передаёт Trainer в compute_metrics?  \n",
        "Во время вызова trainer.evaluate() или по окончании эпохи evaluation_strategy=\"epoch\", Trainer вычисляет предсказания и передаёт их в compute_metrics.  \n",
        "📌 Аргумент `eval_preds` внутри `compute_metrics(eval_preds)` – это кортеж: `logits, labels = eval_preds`  \n",
        "Где:  \n",
        "- `logits` – массив логитов модели (размерность (num_examples, num_labels)).  \n",
        "- `labels` – истинные метки (размерность (num_examples,)).  \n",
        "\n",
        "Пример eval_preds:  \n",
        "```\n",
        "np.array([[2.1, 0.3], [0.1, 3.2], [1.2, 1.8]]),  # Логиты (num_examples, num_labels)\n",
        "np.array([0, 1, 1])  # Метки (num_examples,)\n",
        "```"
      ],
      "metadata": {
        "id": "ppsYK24fobhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Создаем новый TrainingArguments"
      ],
      "metadata": {
        "id": "Bdhh_oEgsFsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем новый TrainingArguments с его evaluation_strategy, установленным на \"epoch\", и новую модель (в противном случае мы бы просто продолжили обучение модели, которую мы уже обучили)."
      ],
      "metadata": {
        "id": "IBsGMd3T4fp3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o37DpIZGmnoB"
      },
      "outputs": [],
      "source": [
        "training_args_2 = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\",  report_to=\"none\")\n",
        "model_2 = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "trainer_2 = Trainer(\n",
        "    model_2,\n",
        "    training_args_2,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,  # Передаём функцию, но не вызываем её!\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " `\"test-trainer\"` – это название директории, куда Trainer сохранит файлы (можно поменять название).  \n",
        "📌 Что хранится в \"test-trainer\"?  \n",
        "Когда ты запускаешь trainer.train(), Trainer автоматически создаёт в \"test-trainer\":  \n",
        "- Сохранённые веса модели (pytorch_model.bin)  \n",
        "- Конфиг модели (config.json)  \n",
        "- Логи тренировки (trainer_state.json)  \n",
        "- Аргументы обучения (training_args.bin)  \n",
        "- Tokenizer (если Trainer использует токенизатор)  \n",
        "- Снимки (checkpoint) модели (если включены сохранения)  "
      ],
      "metadata": {
        "id": "Rk35iEyStxIL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "LIX0kOtwmnoB",
        "outputId": "0f239dc1-9d30-4fd4-e475-999d4584e2b3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1377/1377 03:29, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.371126</td>\n",
              "      <td>0.857843</td>\n",
              "      <td>0.901024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.527200</td>\n",
              "      <td>0.459970</td>\n",
              "      <td>0.845588</td>\n",
              "      <td>0.893401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.279600</td>\n",
              "      <td>0.774146</td>\n",
              "      <td>0.848039</td>\n",
              "      <td>0.894915</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1377, training_loss=0.3341553964386319, metrics={'train_runtime': 209.3512, 'train_samples_per_second': 52.562, 'train_steps_per_second': 6.577, 'total_flos': 405114969714960.0, 'train_loss': 0.3341553964386319, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "trainer_2.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здесь тонкая настройка выполнена с использованием API Trainer (см. примеры в гл. 7 курса). В 8 части сделаем то же в чистом PyTorch."
      ],
      "metadata": {
        "id": "8NXqSnSLQGyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Как загрузить уже обученную модель?"
      ],
      "metadata": {
        "id": "_tuGpEtGu-wk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "После обучения, Trainer сохраняет файлы в папке \"test-trainer\" (или другой, которую ты указал). Теперь мы можем загрузить модель обратно:"
      ],
      "metadata": {
        "id": "clByYUGRvVSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "checkpoint = \"test-trainer\"  # Путь к сохранённой модели"
      ],
      "metadata": {
        "id": "4sy8Qt-dvOCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Загружаем модель и токенизатор"
      ],
      "metadata": {
        "id": "PxCrB1ABu5-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_load = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "tokenizer_load = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "S-ypK9G1vjYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 Что здесь происходит?  \n",
        "`from_pretrained(checkpoint)` - загружает модель из указанной папки (test-trainer).  \n",
        "- Веса модели (`pytorch_model.bin)`,\n",
        "- конфиг (`config.json`) и токенизатор загружаются автоматически.  "
      ],
      "metadata": {
        "id": "7l5rjRryvMLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2️⃣ Как сделать предсказания?"
      ],
      "metadata": {
        "id": "jk_6YZGHv90K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Не очевидный пример, но ...\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=model_load, tokenizer=tokenizer_load)\n",
        "\n",
        "text = \"This is an amazing example!\"\n",
        "result = classifier(text)\n",
        "print(result)\n",
        "# Вывод будет примерно таким:\n",
        "# [{'label': 'LABEL_1', 'score': 0.98}]\n",
        "# 📌 В зависимости от модели, метки (LABEL_0, LABEL_1)\n",
        "# могут соответствовать разным классам\n",
        "#  (например, \"not paraphrase\" и \"paraphrase\" в MRPC)."
      ],
      "metadata": {
        "id": "rt5OytJpw0XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "🔄 3️⃣ Как дообучить модель?"
      ],
      "metadata": {
        "id": "t6CCGhF2wMYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_load = Trainer(\n",
        "    model=model_load,  # Используем уже обученную модель\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer_load,\n",
        "    compute_metrics=compute_metrics, # Если мы ее предварительно создали (и под эту модель!)\n",
        ")"
      ],
      "metadata": {
        "id": "Lgi0l_uBxD8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()  # Продолжаем обучение!"
      ],
      "metadata": {
        "id": "JOXghYQHxjlw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "bA413B7lp7mC",
        "o0ZdAjnbrqxz",
        "TUSxJE3Br8CF"
      ]
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "75895baca542443b953e535da6fba412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ace11875170457cb6a39a9c501ef154",
              "IPY_MODEL_a7892a95fe604e359d9274b21566a3ef",
              "IPY_MODEL_8ae8c7a157d24af4a9db6a0a5139e1ae"
            ],
            "layout": "IPY_MODEL_a7b6d0198d6d4ddb92c1d5e633756811"
          }
        },
        "0ace11875170457cb6a39a9c501ef154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b456a04d1a546749144b0acfd876cd4",
            "placeholder": "​",
            "style": "IPY_MODEL_7dcd678364ef4e378d138715933de386",
            "value": "Map: 100%"
          }
        },
        "a7892a95fe604e359d9274b21566a3ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a125ae2bf3f14795a948f592d50c9e52",
            "max": 408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12cc076a926d4b3992af57cc59682873",
            "value": 408
          }
        },
        "8ae8c7a157d24af4a9db6a0a5139e1ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd1099ebe5e14cb8a60ba03423d4fef6",
            "placeholder": "​",
            "style": "IPY_MODEL_b4bc416f260b47c888dc5b8b6e44ea0f",
            "value": " 408/408 [00:00&lt;00:00, 3797.32 examples/s]"
          }
        },
        "a7b6d0198d6d4ddb92c1d5e633756811": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b456a04d1a546749144b0acfd876cd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dcd678364ef4e378d138715933de386": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a125ae2bf3f14795a948f592d50c9e52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12cc076a926d4b3992af57cc59682873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd1099ebe5e14cb8a60ba03423d4fef6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4bc416f260b47c888dc5b8b6e44ea0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}