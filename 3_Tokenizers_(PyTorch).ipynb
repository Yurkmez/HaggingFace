{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQWmWw7a8_kB"
      },
      "source": [
        "# Tokenizers (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khvAuLzT8_kD"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcX1wyDW8_kE"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KMMtcW48_kG",
        "outputId": "02ce661c-67ed-49aa-e1d4-d243da08034b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
          ]
        }
      ],
      "source": [
        "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwkjVjnf9mbU"
      },
      "source": [
        "## Загрузка и сохранение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2L3bxBs9ofC"
      },
      "source": [
        "Загрузка и сохранение токенизаторов так же просты, как и в случае с моделями: они основаны на тех же двух методах: `from_pretrained()` и `save_pretrained()`. Эти методы загрузят или сохранят алгоритм, используемый токенизатором (немного похоже на архитектуру модели), а также его словарь (немного похоже на веса модели)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qeTh5V2BIG8"
      },
      "source": [
        "### Загрузка токенизатора"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5M3jTPpBQrx"
      },
      "source": [
        "Загрузка токенизатора BERT, обученного с той же контрольной точкой, что и BERT, выполняется так же, как загрузка модели, за исключением того, что мы используем класс BertTokenizer, но что эквивалентно, загрузка спомощью класса AutoTokenizer выберет нужный класс токенизатора в библиотеке на основе имени контрольной точки и т.о., может использоваться напрямую с любой контрольной точкой:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w-7WEnc48_kH"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "# from transformers import AutoTokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnQj6qOcA3Kr"
      },
      "source": [
        "### Сохранение токенизатора"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Сохранение в Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TS5rj1R_BY4",
        "outputId": "32d386fd-c53c-4b33-bcd0-dafb984ba303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "any3vq5v_GeB"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/My Drive/Hugging_face/tokenizer_dir\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7av9his8_kK",
        "outputId": "b7b40c60-daa9-40de-d29c-de594f1bdef0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/Hugging_face/tokenizer_dir/tokenizer_config.json',\n",
              " '/content/drive/My Drive/Hugging_face/tokenizer_dir/special_tokens_map.json',\n",
              " '/content/drive/My Drive/Hugging_face/tokenizer_dir/vocab.txt',\n",
              " '/content/drive/My Drive/Hugging_face/tokenizer_dir/added_tokens.json')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.save_pretrained(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGDXo3gz_fBN",
        "outputId": "38035282-643b-431c-f6ab-e6b0ba5c4fcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "special_tokens_map.json  tokenizer_config.json  vocab.txt\n"
          ]
        }
      ],
      "source": [
        "ls \"/content/drive/My Drive/Hugging_face/tokenizer_dir\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Сохранение на локальном компьютере (сохраним в той же директории, что и аналогичная модель)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('bert-base-cased_model_saved\\\\tokenizer_config.json',\n",
              " 'bert-base-cased_model_saved\\\\special_tokens_map.json',\n",
              " 'bert-base-cased_model_saved\\\\vocab.txt',\n",
              " 'bert-base-cased_model_saved\\\\added_tokens.json')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.save_pretrained(\"bert-base-cased_model_saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer_2 = AutoTokenizer.from_pretrained(\"bert-base-cased_model_saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq7UQghBBuhU"
      },
      "source": [
        "### Токенизация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW8eb_j-8_kK",
        "outputId": "cad6c51e-d1ab-4673-df82-91dd43cbdd13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens - ['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n",
            "tokenizer_2(sequence) - {'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "ids - [7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
          ]
        }
      ],
      "source": [
        "sequence = \"Using a Transformer network is simple\"\n",
        "\n",
        "# Вначаеле идет собственно токкенизация\n",
        "tokens = tokenizer_2.tokenize(sequence)\n",
        "print(f'tokens - {tokens}')\n",
        "\n",
        "# а затем эти токены индексируются в соответствии с индексами соответсвующей им модели\n",
        "print(f'tokenizer_2(sequence) - {tokenizer_2(sequence)}')\n",
        "\n",
        "ids = tokenizer_2.convert_tokens_to_ids(tokens)\n",
        "print(f'ids - {ids}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "m-6CrNj48_kM",
        "outputId": "fd8cbd60-32b7-4f26-83d4-09aa5f313856"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Using a transformer network is simple'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
        "decoded_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Обратите внимание, что метод декодирования не только преобразует индексы обратно в токены, но и группирует токены, которые были частью тех же слов, чтобы создать читаемое предложение. Такое поведение будет чрезвычайно полезным, когда мы используем модели, которые предсказывают новый текст (либо текст, сгенерированный из подсказки, либо для задач последовательности-последовательности, таких как перевод или резюмирование).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Для подачи в модель, надо взять тензоры"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "model = AutoModel.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([  101,  7993,   170, 13809, 23763,  2443,  1110,  3014,   102])\n",
            "tensor([ 7993,   170, 13809, 23763,  2443,  1110,  3014])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "model_inputs_1 = torch.tensor(tokenizer_2(sequence)[\"input_ids\"])\n",
        "model_inputs_2 = torch.tensor(ids)\n",
        "print(model_inputs_1)\n",
        "print(model_inputs_2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tokenizers (PyTorch)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
